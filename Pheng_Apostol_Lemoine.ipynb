{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pheng_Apostol_Lemoine.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rospote/machine_learning/blob/main/Pheng_Apostol_Lemoine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oL2OSXqxpbr"
      },
      "source": [
        "#Exercise 1.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7S3HKV60BIp"
      },
      "source": [
        "#Stochastic gradient descent optimizer and mean square error as the loss calculator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jA2ab-0xofM",
        "outputId": "6fa862cd-1a66-499b-f3a4-28064a154495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "# Creating a sequential model\n",
        "model = Sequential()\n",
        "model.add(Dense(4, activation='relu', input_shape=(3,)))\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "\n",
        "# compiling the model\n",
        "model.compile(loss='mse',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nwUK2j40BV3"
      },
      "source": [
        "#SGD optimizer using a learning rate of 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWleLrdE0tcy"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Creating a sequential model\n",
        "model = Sequential()\n",
        "model.add(Dense(4, activation='relu', input_shape=(3,)))\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "\n",
        "# compiling the model\n",
        "sgd = SGD(lr=0.01)\n",
        "model.compile(loss='mean_squared_error', optimizer=sgd,    \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnxI_mOm0CIg"
      },
      "source": [
        "#MNIST and Sequential model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXfls-74cb3V"
      },
      "source": [
        "###Code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBjgqHYG0FY6",
        "outputId": "33e1b0a6-f073-4394-a50f-b87c5cfd63ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Trains a simple deep NN on the MNIST dataset.\n",
        "Gets to 98.40% test accuracy after 20 epochs\n",
        "(there is *a lot* of margin for parameter tuning).\n",
        "2 seconds per epoch on a K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.2477 - accuracy: 0.9238 - val_loss: 0.1086 - val_accuracy: 0.9658\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 11s 175us/sample - loss: 0.1021 - accuracy: 0.9687 - val_loss: 0.0893 - val_accuracy: 0.9726\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 11s 181us/sample - loss: 0.0762 - accuracy: 0.9770 - val_loss: 0.0701 - val_accuracy: 0.9797\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 10s 165us/sample - loss: 0.0610 - accuracy: 0.9817 - val_loss: 0.0831 - val_accuracy: 0.9755\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.0476 - accuracy: 0.9859 - val_loss: 0.0741 - val_accuracy: 0.9795\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 10s 170us/sample - loss: 0.0435 - accuracy: 0.9870 - val_loss: 0.0785 - val_accuracy: 0.9810\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.0384 - accuracy: 0.9880 - val_loss: 0.0760 - val_accuracy: 0.9816\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 10s 166us/sample - loss: 0.0356 - accuracy: 0.9894 - val_loss: 0.0817 - val_accuracy: 0.9810\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 10s 170us/sample - loss: 0.0298 - accuracy: 0.9909 - val_loss: 0.0935 - val_accuracy: 0.9819\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 10s 164us/sample - loss: 0.0303 - accuracy: 0.9911 - val_loss: 0.0873 - val_accuracy: 0.9826\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 10s 173us/sample - loss: 0.0280 - accuracy: 0.9921 - val_loss: 0.0830 - val_accuracy: 0.9854\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 10s 165us/sample - loss: 0.0259 - accuracy: 0.9929 - val_loss: 0.1045 - val_accuracy: 0.9818\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0244 - accuracy: 0.9931 - val_loss: 0.1183 - val_accuracy: 0.9789\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 10s 174us/sample - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.1113 - val_accuracy: 0.9820\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 11s 177us/sample - loss: 0.0226 - accuracy: 0.9938 - val_loss: 0.1227 - val_accuracy: 0.9814\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 10s 165us/sample - loss: 0.0226 - accuracy: 0.9939 - val_loss: 0.1322 - val_accuracy: 0.9818\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 10s 168us/sample - loss: 0.0237 - accuracy: 0.9941 - val_loss: 0.1245 - val_accuracy: 0.9811\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 10s 168us/sample - loss: 0.0217 - accuracy: 0.9945 - val_loss: 0.1426 - val_accuracy: 0.9794\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 10s 168us/sample - loss: 0.0201 - accuracy: 0.9947 - val_loss: 0.1358 - val_accuracy: 0.9826\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 11s 180us/sample - loss: 0.0202 - accuracy: 0.9951 - val_loss: 0.1428 - val_accuracy: 0.9826\n",
            "Test loss: 0.1427997176778564\n",
            "Test accuracy: 0.9826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXACw36sS0E1"
      },
      "source": [
        "###Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Mf2TYzS2cC"
      },
      "source": [
        "The goal of the script is to train a simple deep NN on the MNIST dataset.\n",
        "With only the two hidden layers and two dropouts, we can get to 98.26% test accuracy after 20 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_-n2Lcb4Cz4"
      },
      "source": [
        "#Exercise 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSoHSquk4JRz"
      },
      "source": [
        "#MNIST and Convolutional neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTuWQWXucXsZ"
      },
      "source": [
        "###Code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQvXt2xI4MYv",
        "outputId": "6a193e52-d2dd-4892-bbe9-3ed3200622bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "'''Trains a simple convnet on the MNIST dataset.\n",
        "Gets to 99.25% test accuracy after 12 epochs\n",
        "(there is still a lot of margin for parameter tuning).\n",
        "16 seconds per epoch on a GRID K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tensorflow.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 8s 131us/sample - loss: 2.2791 - acc: 0.1612 - val_loss: 2.2490 - val_acc: 0.3944\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 2.2260 - acc: 0.2792 - val_loss: 2.1809 - val_acc: 0.5881\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 2.1554 - acc: 0.3793 - val_loss: 2.0895 - val_acc: 0.6410\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 2.0569 - acc: 0.4523 - val_loss: 1.9653 - val_acc: 0.6733\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 1.9300 - acc: 0.5099 - val_loss: 1.8026 - val_acc: 0.7093\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 1.7706 - acc: 0.5591 - val_loss: 1.6043 - val_acc: 0.7568\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 1.5955 - acc: 0.6012 - val_loss: 1.3943 - val_acc: 0.7905\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 1.4232 - acc: 0.6352 - val_loss: 1.1981 - val_acc: 0.8114\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 1.2727 - acc: 0.6581 - val_loss: 1.0331 - val_acc: 0.8241\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 1.1490 - acc: 0.6798 - val_loss: 0.9039 - val_acc: 0.8334\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 1.0546 - acc: 0.6960 - val_loss: 0.8058 - val_acc: 0.8388\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.9773 - acc: 0.7134 - val_loss: 0.7295 - val_acc: 0.8468\n",
            "Test loss: 0.7295243325233459\n",
            "Test accuracy: 0.8468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhR-VO8dTTd5"
      },
      "source": [
        "###Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUpo4CfTTVHi"
      },
      "source": [
        "Here we change slightly the method. We're keeping the sequential model but we're also adding a 2D convolutional neural networks. This results in a longer computing time per epoch, but we managed to get to 85% test accuracy after 12 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VylU92IS_uhG"
      },
      "source": [
        "#LSTM models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vku8KYh_wtR"
      },
      "source": [
        "##Example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqZpGlAM_0Gi",
        "outputId": "b16f9c68-5641-4769-e1df-17058bbe02a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# LSTM for international airline passengers problem with regression framing\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back, 0])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset\n",
        "dataframe = read_csv('/content/gdrive/My Drive/Colab Notebooks/airline-passengers.csv', usecols=[1], engine='python')\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(1, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
        "# make predictions\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "# invert predictions\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "# plot baseline and predictions\n",
        "plt.plot(scaler.inverse_transform(dataset))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Train on 94 samples\n",
            "Epoch 1/100\n",
            "94/94 - 1s - loss: 0.0568\n",
            "Epoch 2/100\n",
            "94/94 - 0s - loss: 0.0276\n",
            "Epoch 3/100\n",
            "94/94 - 0s - loss: 0.0201\n",
            "Epoch 4/100\n",
            "94/94 - 0s - loss: 0.0183\n",
            "Epoch 5/100\n",
            "94/94 - 0s - loss: 0.0172\n",
            "Epoch 6/100\n",
            "94/94 - 0s - loss: 0.0162\n",
            "Epoch 7/100\n",
            "94/94 - 0s - loss: 0.0152\n",
            "Epoch 8/100\n",
            "94/94 - 0s - loss: 0.0140\n",
            "Epoch 9/100\n",
            "94/94 - 0s - loss: 0.0131\n",
            "Epoch 10/100\n",
            "94/94 - 0s - loss: 0.0120\n",
            "Epoch 11/100\n",
            "94/94 - 0s - loss: 0.0111\n",
            "Epoch 12/100\n",
            "94/94 - 0s - loss: 0.0102\n",
            "Epoch 13/100\n",
            "94/94 - 0s - loss: 0.0092\n",
            "Epoch 14/100\n",
            "94/94 - 0s - loss: 0.0083\n",
            "Epoch 15/100\n",
            "94/94 - 0s - loss: 0.0076\n",
            "Epoch 16/100\n",
            "94/94 - 0s - loss: 0.0067\n",
            "Epoch 17/100\n",
            "94/94 - 0s - loss: 0.0059\n",
            "Epoch 18/100\n",
            "94/94 - 0s - loss: 0.0052\n",
            "Epoch 19/100\n",
            "94/94 - 0s - loss: 0.0046\n",
            "Epoch 20/100\n",
            "94/94 - 0s - loss: 0.0042\n",
            "Epoch 21/100\n",
            "94/94 - 0s - loss: 0.0036\n",
            "Epoch 22/100\n",
            "94/94 - 0s - loss: 0.0033\n",
            "Epoch 23/100\n",
            "94/94 - 0s - loss: 0.0029\n",
            "Epoch 24/100\n",
            "94/94 - 0s - loss: 0.0026\n",
            "Epoch 25/100\n",
            "94/94 - 0s - loss: 0.0026\n",
            "Epoch 26/100\n",
            "94/94 - 0s - loss: 0.0024\n",
            "Epoch 27/100\n",
            "94/94 - 0s - loss: 0.0023\n",
            "Epoch 28/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 29/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 30/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 31/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 32/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 33/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 34/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 35/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 36/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 37/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 38/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 39/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 40/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 41/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 42/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 43/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 44/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 45/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 46/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 47/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 48/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 49/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 50/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 51/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 52/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 53/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 54/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 55/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 56/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 57/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 58/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 59/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 60/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 61/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 62/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 63/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 64/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 65/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 66/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 67/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 68/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 69/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 70/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 71/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 72/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 73/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 74/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 75/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 76/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 77/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 78/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 79/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 80/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 81/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 82/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 83/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 84/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 85/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 86/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 87/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 88/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 89/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 90/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 91/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 92/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 93/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 94/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 95/100\n",
            "94/94 - 0s - loss: 0.0020\n",
            "Epoch 96/100\n",
            "94/94 - 0s - loss: 0.0022\n",
            "Epoch 97/100\n",
            "94/94 - 0s - loss: 0.0019\n",
            "Epoch 98/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 99/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Epoch 100/100\n",
            "94/94 - 0s - loss: 0.0021\n",
            "Train Score: 23.04 RMSE\n",
            "Test Score: 47.36 RMSE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFC90cXJftFbwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxxUbfts_yPK"
      },
      "source": [
        "##Example 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsXuucyrcNgC"
      },
      "source": [
        "###Code: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T3getZoAK6J",
        "outputId": "e2afb4cb-f1eb-42d2-a18f-b5e51aee1670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "#Example script to generate text from Nietzsche's writings.\n",
        "At least 20 epochs are required before the generated text\n",
        "starts sounding coherent.\n",
        "It is recommended to run this script on GPU, as recurrent\n",
        "networks are quite computationally intensive.\n",
        "If you try this script on new data, make sure your corpus\n",
        "has at least ~100k characters. ~1M is better.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "path = get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# build the model: a single LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=60,\n",
        "callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 1us/step\n",
            "corpus length: 600893\n",
            "total chars: 57\n",
            "nb sequences: 200285\n",
            "Vectorization...\n",
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 200285 samples\n",
            "Epoch 1/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 2.0057\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ined class of spirits,\n",
            "who desire almost\"\n",
            "ined class of spirits,\n",
            "who desire almost and as the suppession of the suppossions and as the suppisions and proble and as the suppisious the suppossion of the prosent is the destenting and be the suppossion of the suppossion of the suppossions and fact of the from of the suppossion of the supposse of the suppession of the propress and superion of the suppession and as the probless and supposse and as the disconsentance and morality in t\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ined class of spirits,\n",
            "who desire almost\"\n",
            "ined class of spirits,\n",
            "who desire almost the for its himself, in one as and entistic of artals indees of the posserans the suppicion to as the philosopher and the much as subjuction of the discented in as it is as the great the indeerans, a ful about where in the superions and ssull, the for successions of his stranges, and some does a persaps and do awarn of the conscience and prosence of the apposents they not is the justed the prople\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ined class of spirits,\n",
            "who desire almost\"\n",
            "ined class of spirits,\n",
            "who desire almost why seeming incoustenne y wthe aring, a philosophers. it ippensamanny a prove equal and meand lid add becourainy if the very most for his\n",
            "their\n",
            "phiction of thit\n",
            "fucting of the coses, a treatance, bet senfucianing but is the bedered is is it as a goiins exelfolems and a may, we imat the deplessionaction. he to im that the somicrible\n",
            "to achum\n",
            "tence to concerted say instrowingal pall\n",
            "and lest inplen\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ined class of spirits,\n",
            "who desire almost\"\n",
            "ined class of spirits,\n",
            "who desire almost on as tect we dabeation in\n",
            "potsen-ammonally moraconcaning the epe to comequils and naturive,\n",
            "\"himmoselul otl every gistes and obter, of thesevaction of\n",
            "mordutists and dfincal. his\n",
            "-perions, inso\n",
            "ireed spualant igonly and able, likeed is unturmanion, which a beare is is thiokfiling, of reim\n",
            "so bestinged alvalorser. and prossion. thand\" beent reaval worked tyan i had. gistadies of\n",
            "naturious? fur-ie\n",
            "200285/200285 [==============================] - 117s 586us/sample - loss: 2.0056\n",
            "Epoch 2/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.6560\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"propagate themselves. one must appeal to\"\n",
            "propagate themselves. one must appeal to the same and so a say of the same and contrasian and in the still and his such a comples and artituage and a man and so completing the such a man and soul and present and in the self-so an and and in the present and still the such a more and as a man the compless and develop the sentiments of the distress and appear and his present and in the still and he will and and and are and an a philosopher\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"propagate themselves. one must appeal to\"\n",
            "propagate themselves. one must appeal to the such a prence. when he friend in the existed to the presist of the lecres and man of the the worth the soul of the hards and imperfore the standing as its more of a carres of existed to the such his man neight the wart from the comman\n",
            "from the one which it is precession of the sich the form and instinct the and in the discrense and the order as a madent when the\n",
            "work of\n",
            "a maden and into do an\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"propagate themselves. one must appeal to\"\n",
            "propagate themselves. one must appeal to pnicumation and apon very nature onlyt thom\n",
            "it slood backne, not degerm\n",
            "cilept in ourself.--a colder. as how a curanges: stchilogt, \"the repreding\n",
            "; as intain the sably ar-alfort, anvark\n",
            "of the the allems symplecial eviluing, orever. in corribles you here iverternti lugh to him, ouncamaact, greem of greater\" of thesecomer which one\n",
            "bedient the sensatenes from this yout eartly with ware and histow\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"propagate themselves. one must appeal to\"\n",
            "propagate themselves. one must appeal toganimel-psea\n",
            "grranghient commay contasp in? inevery just frenthexhatious hupence from one love.=--it ourlowed--power,\n",
            "synal mitradever--consequent, man, oues\n",
            "but orual potterful, it usonlieft hame,amy will or thqura-glym--that sh overche, and life, how\n",
            "a\n",
            "restics? is a attolent diffuntanged by,\"bard; ard thysely anisming of philosopherygy\n",
            "bearth.asts spribdom conclumsance of he intempt we so\n",
            "cor lo\n",
            "200285/200285 [==============================] - 111s 555us/sample - loss: 1.6560\n",
            "Epoch 3/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.5665\n",
            "----- Generating text after Epoch: 2\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"d hence\n",
            "affords a clue to the understand\"\n",
            "d hence\n",
            "affords a clue to the understanding and the stronger and the condition of the sense of the present and the spirit of the same and in the extent and as the standards and as a strength of their and in the strength and the extent and the stronger of the stronger and in the strange and the sense of the spiritual and in the strength and the strength of the fact of the strength of the strength of the scholly and as the spirit and the \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"d hence\n",
            "affords a clue to the understand\"\n",
            "d hence\n",
            "affords a clue to the understanding the wish all the extention of a canned prosedical and inspitance of the deligious prosest bature of the extention the consequently as the schoses the sperious not it is others and as it is the world and are experience of the world and more that sense and is in their pranifice of man\" which\n",
            "he must as a philosophy, as else and as a self-schother indical experience, the chance as an in self-and \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"d hence\n",
            "affords a clue to the understand\"\n",
            "d hence\n",
            "affords a clue to the understanding aaddif connection, than not humanent intellectual\n",
            "sithing\n",
            "former my extens for their \"masis toed the narny and procass in the sabgreally\n",
            "ogier\n",
            "as looked as herlical nowampo sequents as possibly renetes, whater he dehigl obsespess of trativecto-nard-compuls.\n",
            "\n",
            "2lotital formering mole aton can asdu and in \"in is the\n",
            "sense of relation, must can present pittable the necess of\n",
            "conting hame, listen m\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"d hence\n",
            "affords a clue to the understand\"\n",
            "d hence\n",
            "affords a clue to the understandine but a my was\n",
            "litty of with a relaratudionduuss, are partifuen,\n",
            "by graty has cun danger alshay sy, when the existemn.\n",
            "\n",
            "\n",
            "\n",
            "yrepnestratwical  was,\n",
            "have,\n",
            "brauns founnglbitym-thing-mading tongered thave\n",
            "me\",\n",
            "is sentinable lave engangated to melkrekcint look, non those its should, ever nypschated to the\n",
            "moration, than\n",
            ". it have judgments, must legdiby. as\n",
            "intenceasion of\n",
            "spe.ful. in like\n",
            "man my infli\n",
            "200285/200285 [==============================] - 111s 552us/sample - loss: 1.5664\n",
            "Epoch 4/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.5205\n",
            "----- Generating text after Epoch: 3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" was pronounced a person of whom\n",
            "society\"\n",
            " was pronounced a person of whom\n",
            "society and and the self and the great development and present and the intellectual and the point which the problem and the sense and the last and the sense and the sense and and intention of the sense and and the self the sense and the sense and self and the self and the self and the sense and the same the and the self and the present and the prejenour and the self and the the every personal prejent and\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" was pronounced a person of whom\n",
            "society\"\n",
            " was pronounced a person of whom\n",
            "society and deveneation of the contempting the religious and moral,\n",
            "the instinction in the stuls is a species the\n",
            "problemation, and the because to the last the heart the present and the sperious specialing the possible and the maning the person the conception of a new fact happiness, the forlithed who have a stands the\n",
            "and the every conceptions, and a subject and present necessary no most the coundation \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" was pronounced a person of whom\n",
            "society\"\n",
            " was pronounced a person of whom\n",
            "society itself-philosopher prejeation of\n",
            "sangation: must the fearging and empt intellectual tradive self\n",
            "than the berimpour, not everything and sublimates take-somethings ripenation of the period of jampargogian exprosent, regimstwnhing-mament,\" sthogaly), a sensegiation, but every spu good to reoustesit of\n",
            "incaterared the let others which the art it yrudy.\n",
            "\n",
            "219. \n",
            "urtunable to done the brings of the leas\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" was pronounced a person of whom\n",
            "society\"\n",
            " was pronounced a person of whom\n",
            "society, philosopher fan his genotiveative facting incornom of real\n",
            "adiveradtieveal wi nwilang the conceptuarian af means-inve a-translaties urttets and tyen\n",
            "so, that iterspourimental iscries. hnel\n",
            "of necessably sataite,--that\n",
            "in a ruling in\n",
            "this fin fact havk intellence, what have not i comntrofthen) knownish: see in nom, of learring from the interniation the virtaicy \n",
            " ecen of religional nereoshod? her\n",
            "200285/200285 [==============================] - 111s 552us/sample - loss: 1.5206\n",
            "Epoch 5/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4918\n",
            "----- Generating text after Epoch: 4\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"nd\n",
            "dissembling power (his \"spirit\") had \"\n",
            "nd\n",
            "dissembling power (his \"spirit\") had in the stilly the same the stands in the same the morality and reason in the proceted and stand the same and stand the same the state of the present in the spirit and stand the state of the servation of the spirits of the spirit of the morality and reason in the morality and present the state of the probably and and and in the probably the same the state and with the same and the probably all the \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"nd\n",
            "dissembling power (his \"spirit\") had \"\n",
            "nd\n",
            "dissembling power (his \"spirit\") had to these is the interpretation of the christian morality, as the most metaphysical perhaps the interpretation of an inmoralihe existence thereby\" the presument and reaching this presurility and more still the precisely, as all these are still to involuntarility--he regard to a perhaps and fundamental and stopet and in the free world probably such may every stappoination of the moral\n",
            "process of his\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"nd\n",
            "dissembling power (his \"spirit\") had \"\n",
            "nd\n",
            "dissembling power (his \"spirit\") had for\n",
            "nowadienge espusic. to one themselvesn; there is it taken bearnoscrevers,\" and it is it nobour. than where as in the paungly stands, my there is the pokents of the most any beautisply offlick thew with mas it is mustabidies, in the handuresing tygened him; and\n",
            "first in long out wotle, every \"the longer, ne vality the righl, mistaken that existent, and is noble their render servoges, with\n",
            "profo\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"nd\n",
            "dissembling power (his \"spirit\") had \"\n",
            "nd\n",
            "dissembling power (his \"spirit\") had unour pspabe imaully, wellowly \"and den opitabe that it have for theirselattedt--that and\n",
            "his man's filleness, any saitily, justerl\n",
            "stifnedidly,\n",
            "it most has sunder clumerinally werion of \"gimity in\n",
            "anipultationiplise customa.\n",
            "aacon interpretaten too the\n",
            "despictorn themselves advantary. wat nownast-vappainite\n",
            "\"from himself\n",
            "mea mightorsh discoverourol highest letvee: heroup, broad above one by lacka\n",
            "200285/200285 [==============================] - 111s 553us/sample - loss: 1.4918\n",
            "Epoch 6/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4723\n",
            "----- Generating text after Epoch: 5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ways become in the end, even under the\n",
            "m\"\n",
            "ways become in the end, even under the\n",
            "man in the senses and personal preservation of the speak of the sense and success and sense of the sublimal and soul of the sense and sublimatical and a sense and result and man is and more very and sense and fact of the sense of the reason of the sense of the sense and standard and present and strength and problem and strength and result and standard suspicial problem and present and soul of the s\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ways become in the end, even under the\n",
            "m\"\n",
            "ways become in the end, even under the\n",
            "man, as the being who lind the articully the souls explais of which reason in success and englished to suspicial soul, a condition in the sublimation, and preached and stand all the same truth and respect to secrece and the bely consequence all that referious very be and stranger, and standard consistion, a desire and according the presubly by as to presuble that the prinities the life to suspicial\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ways become in the end, even under the\n",
            "m\"\n",
            "ways become in the end, even under the\n",
            "most,\n",
            "what is to be toast and ricud a sucis and spiritual impercess at an orutualeges who to be gape-\"yet rip and case plade of the outselg tarng bell, averay sly self-sets\n",
            "womeruthe him as festions\"; reaged, however hardauptiess, powerable blessions, attrycvative resunce, with hitherto allowadenic surbumial keemat--in himself\n",
            "enoold\n",
            "cressess\"? that any changaiture, whete purposenties is the ear pr\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ways become in the end, even under the\n",
            "m\"\n",
            "ways become in the end, even under the\n",
            "many the judge, rempill; just luid him\n",
            "malinestly\n",
            "wehfuoked there\" it gratually right; evil couresy -halked gidraind yanger on ergoss itself(ablum\n",
            "very of varerict--enforme, hreas: thereof. try expegsion,\" the unifire considselc.\n",
            "\n",
            "noigious dange"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rry these delighed\n",
            "hangemal,\n",
            "ovitific orumal and rsopy givers, butly, and my conclets gorransoud; eoolity of stre,\n",
            "boeer, mean orfige with and \"mudid mere a\n",
            "200285/200285 [==============================] - 111s 552us/sample - loss: 1.4724\n",
            "Epoch 7/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4588\n",
            "----- Generating text after Epoch: 6\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"iment of sin is dulled when it is percei\"\n",
            "iment of sin is dulled when it is perceive to the conscience of the present to the process of the sense of the conscience of the procultion of the present and sense of the master to the free conscience of all the properson of the present to the truthfulness of the sense of the master to the master to the morals with the present to the master and the present to the conscience of the sentiments of the conscience of the present in the cons\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"iment of sin is dulled when it is percei\"\n",
            "iment of sin is dulled when it is perceive this awaken in the first nature of the pression, and the whole also an age present experience of an interpritt to the will may be\n",
            "the subject to we may we for some and such a determinity of men to the master to the particuses and suspicior of this presentahcown to inserve to man in the conduct\n",
            "of everything who have self,\n",
            "and the proud to the valing in the taste of the power the interpretations\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"iment of sin is dulled when it is percei\"\n",
            "iment of sin is dulled when it is perceiveventay uncestemy read may revence, these mustocroth. even to the ptymisaned to decusion rej.sing coster, ina.\n",
            "but i subjection of precess this aed and whatever ourselves and senerative by we is heurts slur of femonn, to the blunaesr atteque as in the interpretations, were indeed in grain, bower good, were emotions as so\n",
            "nith\n",
            "of othinate sagniness to ungen-aut be burdness reven, i it be skeen ass\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"iment of sin is dulled when it is percei\"\n",
            "iment of sin is dulled when it is perceive too cornoniatitistss) they would thus\n",
            "molene that also be sogene has oc;awitied\n",
            "torkungied plaisulit the _sexuaulumest transformeud.\n",
            "\n",
            "consice\n",
            "upe, of bocherpow\n",
            "works kind as\n",
            "creator\n",
            "for shoves, it is they must,\n",
            "not im raisimum, or repres\n",
            "unsentjean to over taesed so groued, onner marsidelfient to slight xwawcgl nage, of\n",
            "still vi.kindeservackneldite subjective thing of its\"\n",
            "hitherto dappears nre\n",
            "200285/200285 [==============================] - 111s 553us/sample - loss: 1.4587\n",
            "Epoch 8/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4463\n",
            "----- Generating text after Epoch: 7\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"and it is\n",
            "worth while doing so, even if \"\n",
            "and it is\n",
            "worth while doing so, even if the higher and eventing the same and which is the state of the sentiment of the women the state of the suffering the same the state of the sentiment of the same and the later that there is the happiness of the same and the sentiment--that it is the interpretations of the proposition of the strength, and and as a problem in the sense of the proposition and the end with the world of the sentiment of\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"and it is\n",
            "worth while doing so, even if \"\n",
            "and it is\n",
            "worth while doing so, even if down all to \"man is the soul as to a power secret of proper and more wild the others and their both of there--was the simply this for this same spiritual possible as the self and an an intellect the existed the and an allow that there and will aled noble\n",
            "standard the being the more of\n",
            "all think we was a now of the philosophers of the high that; man doen their reason is the such as seems and the be\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"and it is\n",
            "worth while doing so, even if \"\n",
            "and it is\n",
            "worth while doing so, even if dopull; inde grows all gasing than now they was their long such a essential highing and cisuacing of securen's it? or the turning in which it will quites.\"\n",
            "in theever, this a virtuatic vanity of in the blow over suposs spirstic motholity of reinticism-and time, theres thus concection of by predititytic dividual! but ridic-envyion. \"others \"changess of state of own--doearque on woman\n",
            "is it not hidd\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"and it is\n",
            "worth while doing so, even if \"\n",
            "and it is\n",
            "worth while doing so, even if comapliken generad man from at plecievicate wosepine brint and gopproked dneuring;--that theirmalful herselfe douny-same hem\"forwin? who midity,\" woman and divine s her undend only philosophy\"?\n",
            "eesictive of\n",
            "this of\n",
            "this on dread\n",
            "cingueng life\" that had tood gren pri,a\"ed\n",
            "crotomedness. asvolitiority,\" bactn, art or\n",
            "his giness certain\n",
            "cale of\n",
            "whol circulate--oven cannot e;of himself comeul, only ppo\n",
            "200285/200285 [==============================] - 110s 551us/sample - loss: 1.4463\n",
            "Epoch 9/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4353\n",
            "----- Generating text after Epoch: 8\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"n principle;\n",
            "they are always in favour o\"\n",
            "n principle;\n",
            "they are always in favour of the standard and the to the standard and believed to the stranged and which is to the sense, and the standard the sense of the sense of the sense of the standard and sense of the standard to the sense of the stranged to the great contempess of the strength of the sense of the standard and the standard in the stronger and standard and the standard to the sense of the stranged to the soul of the s\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"n principle;\n",
            "they are always in favour o\"\n",
            "n principle;\n",
            "they are always in favour of the fore by such a concerned in the tastes and has been his should not the delicate the and expression, the stands as a music is to him of the holigent, and the experiences of an has the sense, and to the term. the standard and contempl, and the most logical for the from the self-does the not the changed, and the pertains and so grow\n",
            "and the interested that the fact that seems in the strength. t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"n principle;\n",
            "they are always in favour o\"\n",
            "n principle;\n",
            "they are always in favour or is so. problogicated, peoples precise: cateation, and that our effect, and ue; sure friend not many,\"ay\"\n",
            "possible a prothen!\n",
            "\n",
            "wither himself with with perhaps appearen doterous--sirenian even since; is denacon,\n",
            "with as per ifice upine to sufceelf! which it is action feel at the philosophers\n",
            "historical slife\" resenfimentra, of it\n",
            "them, within\n",
            "world?\" meanthen\n",
            "history of this\n",
            "terrolent--the great \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"n principle;\n",
            "they are always in favour o\"\n",
            "n principle;\n",
            "they are always in favour other cultume) aking. threated e. back ir believes and knowledge, nashum band it grettesr, i heed\n",
            "cosemly as his sensual of todies (took on light. \n",
            "new as to many emblas, wail just a sat were but order a\n",
            "oppositionatilipision stide putupre\"?h of the intercountemongly, muse, the\n",
            "=the necessisery out than all time which\n",
            "thinw-other.e-schole in en high soppeness nature\" ficated incimely mustne ofe--pa\n",
            "200285/200285 [==============================] - 110s 549us/sample - loss: 1.4353\n",
            "Epoch 10/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4282\n",
            "----- Generating text after Epoch: 9\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"es: something that goes its way like a l\"\n",
            "es: something that goes its way like a like the spectation of the same the same and the spirit and the self-man who is and a sense of the same the same and the same the sense of the sense of the self-like the spirit the same and the same of the self-desires of the same and the same with desired the same the spirit of the problem of the sense of the specially the strength of the self-deceived the self-deceived the same and the self-man w\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"es: something that goes its way like a l\"\n",
            "es: something that goes its way like a lain the sease of the proposing suffering in a things itself concern and the case of an as any have mean in the world, the has been when which is the one's persones\n",
            "with the common individual fact, the tradeble man in a suffering; and the the displess of the most man is belong to seeks to say the fact, and the work their new will become and commaning and very and little but it would regard powerful\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"es: something that goes its way like a l\"\n",
            "es: something that goes its way like a likeder of all\n",
            "that would a requism. sas as thy ma; not ity, even in discilled rigin afforthing there ours that knowledgisorige--and to some like its evil, to the retcour called as polentle human, has permitle with \"beres penelar sage and evely and accordingrims, as because of powerness, is depths of one's concean-thus,\n",
            "same whichment, the most sees minda, worde his hau wound\n",
            "painfulness, has been \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"es: something that goes its way like a l\"\n",
            "es: something that goes its way like a like she depectivl leact it being ones men pastused in a seed of\n",
            "inarmanb buin a\n",
            "pow diservaged all explaint.\"\n",
            "\n",
            "geæ appurac between made unhater slruthed--or his own taken lat\n",
            "hamvorinable civelo oundebovon the naigic looks motize   ski thing, sea\n",
            "to so mank of the refutual that rather on the same\n",
            "typical himself to powition, in are it proudy one queredimatingnated, oftine-panization: without conti\n",
            "200285/200285 [==============================] - 110s 549us/sample - loss: 1.4284\n",
            "Epoch 11/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4223\n",
            "----- Generating text after Epoch: 10\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"good\" of a community, who have a feeling\"\n",
            "good\" of a community, who have a feeling of the finally and sentiment of the spirits and the probably to the probably the spirits and interpreted the senses to the sense of the forethers and state of the same the state of the fact of the spirits of the sense of the stands the probably the same present and the state of the state of the sense of the suffering, the sentiment to the most one should not itself to the soul of the spirits and \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"good\" of a community, who have a feeling\"\n",
            "good\" of a community, who have a feeling because of\n",
            "manding and to nothing of the present sense of posseatic to the still in all them are no longer spirits\" and a such a same and and some being more present and the sense of the should a suffered and something with as disglor of an end who has to a higher the himself in the antistereble in the conceptions of the still of the\n",
            "interming and the themselves the history of man of the most sim\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"good\" of a community, who have a feeling\"\n",
            "good\" of a community, who have a feeling of \"hatyre of svery vance is and charainst\n",
            "with\n",
            "absolitude, the suffering pripuleds too derre; perhaps ac ?ow\n",
            "progrations is to we authon, perhaps regain when the goetat; the \"yan about there must being predpulate from the hamoy: who dear fairment. the latire good,\" may more inneetified\"--i arrange beens? desires say and polisor, blends it\n",
            "moting\n",
            "and there\"more because on the end arality they hav\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"good\" of a community, who have a feeling\"\n",
            "good\" of a community, who have a feelings, avidies of eet\n",
            "even\n",
            "art at it,\n",
            "whaten, and impulstly catnuatied nowles. only\n",
            "perhaps the soul.\n",
            "\n",
            "esence and a way.\n",
            "\n",
            "\n",
            "25\n",
            "\n",
            "(i otheselity.--we love, re.\n",
            "\n",
            "deëærng will matter stain, peorty--deculations; there is bad, but spiancy!\n",
            "\n",
            "\n",
            "1139. at here.m\n",
            "naish and influends:\n",
            "the pears rare his lacking, duty. others they like--cirn korte into\n",
            "faithch. as\n",
            "or as\n",
            "disconsabilites rounts of forms its specrance a\n",
            "200285/200285 [==============================] - 109s 544us/sample - loss: 1.4222\n",
            "Epoch 12/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4157\n",
            "----- Generating text after Epoch: 11\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"cidental remark of madame de\n",
            "lambert to \"\n",
            "cidental remark of madame de\n",
            "lambert to the subject of the sensuality and the sensuage the most individual and the subject of the sensuality of the present the standard and perhaps a standards and the present and a standard and the proper the subject that it is not be present to the constinent the sensualists and standards in the predition of the subject of the sensuality and still and who have been who have been the process of the worl\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"cidental remark of madame de\n",
            "lambert to \"\n",
            "cidental remark of madame de\n",
            "lambert to the present man some constitutions where he who have to the misunderstanding more only to the principle\n",
            "to its free diservage and constant and condition of the same and the point of the fact of the present the and a religious not to him in the interpretation of the world into itker of the anti-power the world, and the constine, his own dependerating the taste which is a condition, and we power the\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"cidental remark of madame de\n",
            "lambert to \"\n",
            "cidental remark of madame de\n",
            "lambert to his partities--we must addid cannot\n",
            "suco after the world, yew\" by it.--men the tempt to read that of concealgts up hemos a the know what whoever wors and the pa that the evelying that the tate, farm the freedom and as formulation, everyking manferly with the lapedies and lib maintaitity.  opportentss, piochiments--this cause sufferings;\n",
            "l. religious more long and purporties have\n",
            "to be a virtues, i\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"cidental remark of madame de\n",
            "lambert to \"\n",
            "cidental remark of madame de\n",
            "lambert to censeingount\n",
            "hi everything even in easy to the extenta, for by the should woman who irkert. lear as\n",
            "the most in scupte spryness! all become,--look culture,\" that\n",
            "existed to first inwh inclopment, and it?\" tod to morng a judgified, mere of back that we al lead\n",
            "took bewile the perent for alwaiting a phodgorscs! every 2cy ones own it. relief time in contenture\n",
            "natoric diento(blood the\n",
            "inmon unly\n",
            "conc\n",
            "200285/200285 [==============================] - 108s 541us/sample - loss: 1.4156\n",
            "Epoch 13/60\n",
            "200064/200285 [============================>.] - ETA: 0s - loss: 1.4101\n",
            "----- Generating text after Epoch: 12\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"'s desires, not the thing desired.\n",
            "\n",
            "176.\"\n",
            "'s desires, not the thing desired.\n",
            "\n",
            "176. the most most particulation of the most propers and any desire and sense of the serions and more source of the serious morality and sensible that the probably the sense of the strength of the sensations is the sour the soul. the same morality the most period of the same that it is the same and more that the same and the sense of the same the same very and an extent that it is the same personal an\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"'s desires, not the thing desired.\n",
            "\n",
            "176.\"\n",
            "'s desires, not the thing desired.\n",
            "\n",
            "176. the fact, in the most inventive of a morals a sudes\"--it is such a strange is thoughts\n",
            "that which may refined and special\n",
            "and the most party and result of the character of all the present, there is not the same deverous finally and attain by the child and deverition, than a distortion of the specimailess about flow natural and pricated itself so much regard it is to so the master, and we learnt t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"'s desires, not the thing desired.\n",
            "\n",
            "176.\"\n",
            "'s desires, not the thing desired.\n",
            "\n",
            "176. thr\n",
            "xumioue simulare,\n",
            "speak and\n",
            "that merely sesive lather, are sufficrent of pration sympathy mexe he\n",
            "volfruined\"! it affard,\n",
            "that i satima of all that so dang. so discorriips moun over grarmental\n",
            "spirits--resenteds and tastevens and\n",
            "the world-guise, he has no followers a groad. it is facts your their mass\n",
            "age tose fimer no longer men. one should there to whatever ourselves. what over german soun\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"'s desires, not the thing desired.\n",
            "\n",
            "176.\"\n",
            "'s desires, not the thing desired.\n",
            "\n",
            "176. in  spergreg, insertokcen,\n",
            "owass you\n",
            "\"chostricatry, in vieigion,\n",
            "boweuthises,\n",
            "ortang apprisentwound subject\"?  of doem or lake humle\n",
            "themselvesedionsiials? some plejoriam\"bary to vanity\n",
            "sympathy think, disly animal, hearide man. people careic\n",
            "self to lame laved of the cose rarnners--but aneard purp dsive in what, light, yiestally; and to germ of reconclidity of the ex, are nestary\n",
            "veoulihes; it w\n",
            "200285/200285 [==============================] - 109s 542us/sample - loss: 1.4103\n",
            "Epoch 14/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4269\n",
            "----- Generating text after Epoch: 13\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"--it\n",
            "sometimes happens nowadays that a g\"\n",
            "--it\n",
            "sometimes happens nowadays that a great strength and some of the strict of the soul and and and his soul of the problem of the soul, and may be the worth and and the the and the soul, and a such a such as a result of the soul of the soul, and soul of the sense, and and for the such in the contrary, and a such and problem of the problem of the soul of the soul of the suffering of the last and and an any soul of the soul, and and the\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"--it\n",
            "sometimes happens nowadays that a g\"\n",
            "--it\n",
            "sometimes happens nowadays that a great philosophy and being\n",
            "more life the people of the capacity understand the end, however of the the belief in any deceive and action of the problem of the believe--if the species\n",
            "and standed than most the chared, in the cause of the for the personality and the standard into exception of the lange, which is all the distrabed and conscience of the into and consciousness of conception of all condit\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"--it\n",
            "sometimes happens nowadays that a g\"\n",
            "--it\n",
            "sometimes happens nowadays that a guil of his,\n",
            "he which it mote suspocts in de\n",
            "desires that his relation their mende with doon. it has one agemastion. this many\n",
            "satric lofility of case, in the pleasus secreadity--but not the such spirities of errors solitt; thou picted, of do\n",
            "only amszomare, may simpling, nesed will with one, in least them and \"the veived in agheshare, new fear for the highest withy endor, evidence of plato instric\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"--it\n",
            "sometimes happens nowadays that a g\"\n",
            "--it\n",
            "sometimes happens nowadays that a gaze\n",
            "conse\n",
            "\"on it is heaval is\n",
            "though\n",
            "this you kindw\n",
            "hope it in god be excisinv!s\n",
            "of eartipor could itdisteence.\"\n",
            "\n",
            "1iéoppity of fials for indifferent and doven even a  shared in a aware; whoilary rendinging a puritaf.\n",
            "\n",
            "     wruedners peolle. begins wit not grament, the wholded and\n",
            "guilt unin'e racieses to give \"laces there: gor as externies, quicludious improverity. which can seese, and sort has wi\n",
            "200285/200285 [==============================] - 107s 536us/sample - loss: 1.4269\n",
            "Epoch 15/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 1.4414\n",
            "----- Generating text after Epoch: 14\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"hinks\"--even the \"one\" contains an inter\"\n",
            "hinks\"--even the \"one\" contains an intermitt the same and more freedom and the contrart the most all the contemptial the deceive the truth and the most deceive the face and such a present to the same the superficial to the sense\" the fact to the same the as the same truth in the same the same the most such a problem and the the sensation of the same the soul of the will and action of the same the more the same the soul and the contrart \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"hinks\"--even the \"one\" contains an inter\"\n",
            "hinks\"--even the \"one\" contains an interprete to the presussing, happiness of an individual that it does not himself suppricate, and self present and and prescine proper the cure to every distresious question of his highers, from the same the all thought it is a right in the certain in the preservation of the good and contimuration of the fast of the father the all the strength of the conclusion on the same truth of itself\"\n",
            "that the one\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"hinks\"--even the \"one\" contains an inter\"\n",
            "hinks\"--even the \"one\" contains an interpretelyer which in which not perhaps necessarly, insant\n",
            "nature, greatness:--we chopdation of speaks ideas oft what men act appear to seishen to eled to him\n",
            "like, that the\n",
            "will of seems such higher cyll\" for psytobles\n",
            "opeveds of respectal error of\n",
            "the herse\" after there to a stated out of them whether the ascrety and\n",
            "falsary does it is amazivation. the usaxims undermand that the or  prisciple in su\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"hinks\"--even the \"one\" contains an inter\"\n",
            "hinks\"--even the \"one\" contains an interrion is lorals backing flowing for\n",
            "it not aczuable! line\n",
            "\n",
            "\n",
            "\"form of, judness, which has good, and superfical\n",
            "self literackled and is notnes. the during an-sfricek this mexusion;\" sighs\n",
            "from in some! thes paint oft hatroping a thing should have havh also\n",
            "nated.\n",
            "\n",
            "\n",
            "nët-yot gerratelenty--bet not german,\n",
            "our welfing) knimanc matter: there are dalpowing a ficesda, woradness!--but without in the own kill\n",
            "200285/200285 [==============================] - 112s 557us/sample - loss: 1.4414\n",
            "Epoch 16/60\n",
            "200192/200285 [============================>.] - ETA: 0s - loss: 22.8501\n",
            "----- Generating text after Epoch: 15\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"deceptions?\"--that\n",
            "they please--him who \"\n",
            "deceptions?\"--that\n",
            "they please--him who oh we\n",
            "lah hiduallp f tore hedi o athlegfistomalinsdurytn pnsureitlit io annicfasg"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2e5ba7e6776b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m callbacks=[print_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m       \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2e5ba7e6776b>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(epoch, _)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4nWM9ZVpEn"
      },
      "source": [
        "###Notes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wlKJYKvVrlz"
      },
      "source": [
        "In the first example, we create and fit the LSTM network to make predictions with a mean square error loss function.\n",
        "\n",
        "We then invert predictions and calculate the root mean squared error. Finally, we shift the train predictions for plotting.\n",
        "\n",
        "In the second example, we use a bigger network with a categorical crossentropy loss function. This function measures the performance of a classification model whose output is a probability value between 0 and 1. "
      ]
    }
  ]
}